{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxIUWMPaE5vM",
        "outputId": "ff307aa2-3678-446b-865d-abb648f13b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=b0dce1cf8d6eb25000097f6cff425b8a18a27caec4992ee5a7b5ec040291b25d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "from pyspark.sql.types import Row\n",
        "spark=SparkSession.builder.appName('CreateEmptyDataframe').master(\"local[*]\").getOrCreate()\n",
        "schema=StructType([\n",
        "    StructField(\"column1\",StringType(),True),\n",
        "    StructField(\"column2\",IntegerType(),True),\n",
        "    StructField(\"column3\",StringType(),True),\n",
        "])\n",
        "empty_rdd=spark.sparkContext.emptyRDD()\n",
        "empty_df=spark.createDataFrame(empty_rdd,schema)\n",
        "empty_df.printSchema()\n",
        "empty_df.show()\n",
        "empty_df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkhVNQbOFAkj",
        "outputId": "8492b2cb-a82b-4a89-e84d-8c3c74df567f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- column1: string (nullable = true)\n",
            " |-- column2: integer (nullable = true)\n",
            " |-- column3: string (nullable = true)\n",
            "\n",
            "+-------+-------+-------+\n",
            "|column1|column2|column3|\n",
            "+-------+-------+-------+\n",
            "+-------+-------+-------+\n",
            "\n",
            "root\n",
            " |-- column1: string (nullable = true)\n",
            " |-- column2: integer (nullable = true)\n",
            " |-- column3: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename column\n",
        "\n",
        "#rename single column\n",
        "renamed_df = empty_df.withColumnRenamed(\"column1\", \"user_name\")\n",
        "renamed_df.show()\n",
        "\n",
        "#rename multiple columns\n",
        "renamed_df = empty_df.withColumnRenamed(\"column1\", \"user_name\") \\\n",
        "                      .withColumnRenamed(\"column2\", \"user_age\")\n",
        "\n",
        "renamed_df.show()\n",
        "\n",
        "#renaming all\n",
        "df= empty_df.toDF(\"user_name\", \"user_age\", \"user_city\")\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHNHu7uXH_LZ",
        "outputId": "2e29d93e-0f2c-41ee-cde4-94421c78ab9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------+\n",
            "|user_name|column2|column3|\n",
            "+---------+-------+-------+\n",
            "+---------+-------+-------+\n",
            "\n",
            "+---------+--------+-------+\n",
            "|user_name|user_age|column3|\n",
            "+---------+--------+-------+\n",
            "+---------+--------+-------+\n",
            "\n",
            "+---------+--------+---------+\n",
            "|user_name|user_age|user_city|\n",
            "+---------+--------+---------+\n",
            "+---------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=empty_df.toDF(\"user_name\", \"user_age\", \"user_city\")\n",
        "df.show()\n",
        "from pyspark.sql.functions import lit\n",
        "df_with_city = df.withColumn(\"city\", lit(\"Mumbai\"))\n",
        "df_with_city.show()\n",
        "df_with_city.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqGZrW6SNvlq",
        "outputId": "20b4f969-b091-43c5-fcf7-6bf9d0cc9e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---------+\n",
            "|user_name|user_age|user_city|\n",
            "+---------+--------+---------+\n",
            "+---------+--------+---------+\n",
            "\n",
            "+---------+--------+---------+----+\n",
            "|user_name|user_age|user_city|city|\n",
            "+---------+--------+---------+----+\n",
            "+---------+--------+---------+----+\n",
            "\n",
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_age: integer (nullable = true)\n",
            " |-- user_city: string (nullable = true)\n",
            " |-- city: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"preet\",20,\"mulund\"),(\"khusal\",30,\"thane\"),(\"dinesh\",20,\"mumbai\")]\n",
        "df = spark.createDataFrame(data = data, schema = df.columns)\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df2 = df.select(col(\"user_name\"),col(\"user_city\"))\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ZtzYdZRgMr",
        "outputId": "07272d8f-3344-469f-c713-89dfc3e3d536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---------+\n",
            "|user_name|user_age|user_city|\n",
            "+---------+--------+---------+\n",
            "|    preet|      20|   mulund|\n",
            "|   khusal|      30|    thane|\n",
            "|   dinesh|      20|   mumbai|\n",
            "+---------+--------+---------+\n",
            "\n",
            "+---------+---------+\n",
            "|user_name|user_city|\n",
            "+---------+---------+\n",
            "|    preet|   mulund|\n",
            "|   khusal|    thane|\n",
            "|   dinesh|   mumbai|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import cast\n",
        "df.printSchema()\n",
        "df_casted = df.withColumn(\"user_age\", col(\"user_age\").cast(\"string\"))\n",
        "df_casted.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDzgcNOBRrFh",
        "outputId": "58367e87-8c59-498c-fea5-b39c0338473c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_age: long (nullable = true)\n",
            " |-- user_city: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_age: string (nullable = true)\n",
            " |-- user_city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df2 = df.select(col(\"user_name\"),col(\"user_city\"))\n",
        "df2.show()\n",
        "\n",
        "\n",
        "#convert column type\n",
        "from pyspark.sql.functions import cast\n",
        "df.printSchema()\n",
        "df_casted = df.withColumn(\"user_age\", col(\"user_age\").cast(\"string\"))\n",
        "df_casted.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkQl1BB8SqjV",
        "outputId": "be1eb11b-8f5b-4ca1-ec53-337f86947ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|user_name|user_city|\n",
            "+---------+---------+\n",
            "|    preet|   mulund|\n",
            "|   khusal|    thane|\n",
            "|   dinesh|   mumbai|\n",
            "+---------+---------+\n",
            "\n",
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_age: long (nullable = true)\n",
            " |-- user_city: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_age: string (nullable = true)\n",
            " |-- user_city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder .appName(\"PivotUnpivotExample\") \\\n",
        "    .master(\"local[*]\") .getOrCreate()\n",
        "\n",
        "data = [(\"preet\", \"Sales\", 5000,\"Mumbai\"),\n",
        "        (\"rao\", \"Marketing\", 3000,\"Pune\"),\n",
        "        (\"ram\", \"Sales\", 4000,\"Mumbai\"),\n",
        "        (\"ankit\", \"Marketing\", 2000, \"Goa\"),\n",
        "        (\"aniket\", \"HR\", 1000, \"Goa\")]\n",
        "\n",
        "columns = [\"Name\", \"Department\", \"Salary\", \"City\"]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n",
        "pivot_df = df.groupBy(\"City\").pivot(\"Department\").sum(\"Salary\")\n",
        "pivot_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2oC0691UoN8",
        "outputId": "c7921ebc-fbae-4191-9c2e-efa0372d2b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+------+------+\n",
            "|  Name|Department|Salary|  City|\n",
            "+------+----------+------+------+\n",
            "| preet|     Sales|  5000|Mumbai|\n",
            "|   rao| Marketing|  3000|  Pune|\n",
            "|   ram|     Sales|  4000|Mumbai|\n",
            "| ankit| Marketing|  2000|   Goa|\n",
            "|aniket|        HR|  1000|   Goa|\n",
            "+------+----------+------+------+\n",
            "\n",
            "+------+----+---------+-----+\n",
            "|  City|  HR|Marketing|Sales|\n",
            "+------+----+---------+-----+\n",
            "|Mumbai|NULL|     NULL| 9000|\n",
            "|   Goa|1000|     2000| NULL|\n",
            "|  Pune|NULL|     3000| NULL|\n",
            "+------+----+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQaBfQ2xXhrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}